{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=10000, noise=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)), ('rf', RandomFo...f', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=None, voting='hard', weights=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.835\n",
      "RandomForestClassifier 0.849\n",
      "SVC 0.8665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier 0.864\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.842625"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> bag_clf = BaggingClassifier(\n",
    "...     DecisionTreeClassifier(), n_estimators=500,\n",
    "...     bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "...\n",
    ">>> bag_clf.fit(X_train, y_train)\n",
    ">>> bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8505"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.metrics import accuracy_score\n",
    ">>> y_pred = bag_clf.predict(X_test)\n",
    ">>> accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.10307290706412833\n",
      "sepal width (cm) 0.01922010309912284\n",
      "petal length (cm) 0.43643218195171124\n",
      "petal width (cm) 0.44127480788503765\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.datasets import load_iris\n",
    ">>> iris = load_iris()\n",
    ">>> rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    ">>> rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    ">>> for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "...     print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.target = mnist.target.astype(np.uint8)\n",
    "\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rnd_clf.fit(mnist[\"data\"], mnist[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = mpl.cm.hot,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_digit(rnd_clf.feature_importances_)\n",
    "\n",
    "cbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(), rnd_clf.feature_importances_.max()])\n",
    "cbar.ax.set_yticklabels(['Not important', 'Very important'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "          learning_rate=0.5, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n",
    "\n",
    "X_new = np.array([[0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75026781])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=1.0, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=3, n_iter_no_change=None, presort='auto',\n",
       "             random_state=None, subsample=1.0, tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75026781])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=84, n_iter_no_change=None, presort='auto',\n",
       "             random_state=None, subsample=1.0, tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "          for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break  # early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.275313\n",
      "Will train until validation_0-rmse hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-rmse:0.247499\n",
      "[2]\tvalidation_0-rmse:0.222947\n",
      "[3]\tvalidation_0-rmse:0.201049\n",
      "[4]\tvalidation_0-rmse:0.181641\n",
      "[5]\tvalidation_0-rmse:0.166735\n",
      "[6]\tvalidation_0-rmse:0.153621\n",
      "[7]\tvalidation_0-rmse:0.141686\n",
      "[8]\tvalidation_0-rmse:0.131314\n",
      "[9]\tvalidation_0-rmse:0.122047\n",
      "[10]\tvalidation_0-rmse:0.112234\n",
      "[11]\tvalidation_0-rmse:0.10366\n",
      "[12]\tvalidation_0-rmse:0.097232\n",
      "[13]\tvalidation_0-rmse:0.092018\n",
      "[14]\tvalidation_0-rmse:0.087123\n",
      "[15]\tvalidation_0-rmse:0.083105\n",
      "[16]\tvalidation_0-rmse:0.079681\n",
      "[17]\tvalidation_0-rmse:0.076621\n",
      "[18]\tvalidation_0-rmse:0.074141\n",
      "[19]\tvalidation_0-rmse:0.071896\n",
      "[20]\tvalidation_0-rmse:0.070115\n",
      "[21]\tvalidation_0-rmse:0.068281\n",
      "[22]\tvalidation_0-rmse:0.066894\n",
      "[23]\tvalidation_0-rmse:0.065783\n",
      "[24]\tvalidation_0-rmse:0.064704\n",
      "[25]\tvalidation_0-rmse:0.063744\n",
      "[26]\tvalidation_0-rmse:0.062625\n",
      "[27]\tvalidation_0-rmse:0.061978\n",
      "[28]\tvalidation_0-rmse:0.060858\n",
      "[29]\tvalidation_0-rmse:0.059868\n",
      "[30]\tvalidation_0-rmse:0.059202\n",
      "[31]\tvalidation_0-rmse:0.058817\n",
      "[32]\tvalidation_0-rmse:0.058233\n",
      "[33]\tvalidation_0-rmse:0.057706\n",
      "[34]\tvalidation_0-rmse:0.057434\n",
      "[35]\tvalidation_0-rmse:0.056745\n",
      "[36]\tvalidation_0-rmse:0.056413\n",
      "[37]\tvalidation_0-rmse:0.055872\n",
      "[38]\tvalidation_0-rmse:0.055391\n",
      "[39]\tvalidation_0-rmse:0.05512\n",
      "[40]\tvalidation_0-rmse:0.054845\n",
      "[41]\tvalidation_0-rmse:0.054576\n",
      "[42]\tvalidation_0-rmse:0.054318\n",
      "[43]\tvalidation_0-rmse:0.054347\n",
      "[44]\tvalidation_0-rmse:0.054104\n",
      "[45]\tvalidation_0-rmse:0.054106\n",
      "[46]\tvalidation_0-rmse:0.054133\n",
      "Stopping. Best iteration:\n",
      "[44]\tvalidation_0-rmse:0.054104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?\n",
      "\n",
      "A: Yes, if they predict incorrectly on different samples.\n"
     ]
    }
   ],
   "source": [
    "print(\"1. If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?\")\n",
    "print(\"\\nA: Yes, if they predict incorrectly on different samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. What is the difference between hard and soft voting classifiers?\n",
      "\n",
      "A: Hard doesn't pay attention to confidence of model about prediction instead of soft.\n"
     ]
    }
   ],
   "source": [
    "print(\"2. What is the difference between hard and soft voting classifiers?\")\n",
    "print(\"\\nA: Hard doesn't pay attention to confidence of model about prediction instead of soft.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?\n",
      "\n",
      "A: Yes, they can be parallelised, but without boosting, I think.\n"
     ]
    }
   ],
   "source": [
    "print(\"3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?\")\n",
    "print(\"\\nA: Yes, they can be parallelised, but without boosting, I think.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. If your AdaBoost ensemble underfits the training data, what hyperparameters should you tweak and how?\n",
      "\n",
      "A: I will increase learning rate.\n"
     ]
    }
   ],
   "source": [
    "print(\"6. If your AdaBoost ensemble underfits the training data, what hyperparameters should you tweak and how?\")\n",
    "print(\"\\nA: I will increase learning rate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?\n",
      "\n",
      "A: I will decrease learning rate.\n"
     ]
    }
   ],
   "source": [
    "print(\"7. If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?\")\n",
    "print(\"\\nA: I will decrease learning rate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
